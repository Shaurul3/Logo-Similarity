{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "089ad15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"logos.snappy.parquet\")\n",
    "df.to_csv(\"logos.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06943259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 4384/4384 [18:50<00:00,  3.88it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Încarc domeniile\n",
    "df = pd.read_csv(\"logos.csv\")\n",
    "\n",
    "# Costruiesc URL favicon\n",
    "def check_favicon(domain):\n",
    "    try:\n",
    "        favicon_url = f\"https://www.google.com/s2/favicons?sz=64&domain={domain}\"\n",
    "        r = requests.get(favicon_url, timeout=5)\n",
    "        if r.status_code == 200 and r.content:\n",
    "            return favicon_url\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "tqdm.pandas()\n",
    "df[\"logo_url\"] = df[\"domain\"].progress_apply(check_favicon)\n",
    "\n",
    "# Salvare\n",
    "df[[\"domain\", \"logo_url\"]].to_csv(\"favicons_result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27f758bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4011 din 4384 domenii au logo_url (91.49%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"favicons_result.csv\")\n",
    "\n",
    "# Număr total\n",
    "total = len(df)\n",
    "\n",
    "# Număr de favicon-uri găsite (logo_url nu e NaN)\n",
    "found = df[\"logo_url\"].notna().sum()\n",
    "\n",
    "# Afișează\n",
    "print(f\"{found} din {total} domenii au logo_url ({100 * found / total:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4288620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 4384/4384 [2:15:59<00:00,  1.86s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Verifică dacă un URL este valid\n",
    "def is_valid_logo_url(url):\n",
    "    return bool(url)\n",
    "\n",
    "# Verific daca un tag HTML are \"logo\" in el\n",
    "def is_likely_logo(tag):\n",
    "    attributes_to_check = [\"src\", \"srcset\", \"alt\", \"id\", \"data-src\", \"data-original\", \"data-lazy\"]\n",
    "    for attr in attributes_to_check:\n",
    "        val = tag.get(attr)\n",
    "        if val and \"logo\" in val.lower():\n",
    "            return True\n",
    "\n",
    "    # Verific fiecare clasă individual\n",
    "    class_list = tag.get(\"class\")\n",
    "    if class_list:\n",
    "        for cls in class_list:\n",
    "            if \"logo\" in cls.lower():\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# Extrage logo-ul dintr-un HTML static\n",
    "def extract_logo_from_html(base_url, html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Caută <img> care pare \"logo\"\n",
    "    for img in soup.find_all(\"img\"):\n",
    "        if is_likely_logo(img):\n",
    "            src = img.get(\"src\") or img.get(\"srcset\")\n",
    "            full_url = urljoin(base_url, src) if src else None\n",
    "            if is_valid_logo_url(full_url):\n",
    "                return full_url\n",
    "\n",
    "    # Caută <source> (webp etc.)\n",
    "    for source in soup.find_all(\"source\"):\n",
    "        if is_likely_logo(source):\n",
    "            srcset = source.get(\"srcset\")\n",
    "            full_url = urljoin(base_url, srcset) if srcset else None\n",
    "            if is_valid_logo_url(full_url):\n",
    "                return full_url\n",
    "\n",
    "    # Caută background-image: url(...logo...)\n",
    "    for tag in soup.find_all(style=True):\n",
    "        style = tag.get(\"style\", \"\").lower()\n",
    "        if \"background-image\" in style and \"logo\" in style:\n",
    "            match = re.search(r\"url\\((.*?)\\)\", style)\n",
    "            if match:\n",
    "                raw_url = match.group(1).strip('\\'\"')\n",
    "                full_url = urljoin(base_url, raw_url)\n",
    "                if is_valid_logo_url(full_url):\n",
    "                    return full_url\n",
    "\n",
    "    # SVG use xlink:href\n",
    "    for use in soup.find_all(\"use\"):\n",
    "        for attr in [\"xlink:href\", \"href\"]:\n",
    "            href = use.get(attr)\n",
    "            if href and \"logo\" in href.lower():\n",
    "                full_url = urljoin(base_url, href)\n",
    "                if is_valid_logo_url(full_url):\n",
    "                    return full_url\n",
    "\n",
    "    # fallback: caută url(...logo...) brut în HTML\n",
    "    raw_html = soup.prettify().lower()\n",
    "    matches = re.findall(r\"url\\((.*?)\\)\", raw_html)\n",
    "    for m in matches:\n",
    "        if \"logo\" in m:\n",
    "            full_url = urljoin(base_url, m.strip('\\'\"'))\n",
    "            if is_valid_logo_url(full_url):\n",
    "                return full_url\n",
    "\n",
    "    return None\n",
    "\n",
    "# Încearc site-ul simplu și apoi cu www.\n",
    "def extract_logo_smart(domain):\n",
    "    attempts = [f\"https://{domain}\"]\n",
    "    if not domain.startswith(\"www.\"):\n",
    "        attempts.append(f\"https://www.{domain}\")\n",
    "\n",
    "    for base_url in attempts:\n",
    "        try:\n",
    "            r = requests.get(base_url, timeout=12, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "            if r.status_code == 200:\n",
    "                logo_url = extract_logo_from_html(base_url, r.text)\n",
    "                if logo_url:\n",
    "                    return logo_url\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return None\n",
    "\n",
    "# Aplic pe toate domeniile din fișierul CSV\n",
    "df = pd.read_csv(\"logos.csv\")\n",
    "tqdm.pandas()\n",
    "df[\"logo_url\"] = df[\"domain\"].progress_apply(extract_logo_smart)\n",
    "\n",
    "# Salvare\n",
    "df.to_csv(\"logos_detected_HTML.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce99095d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3333 din 4384 domenii au logo_url (76.03%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"logos_detected_HTML.csv\")\n",
    "\n",
    "# Număr total\n",
    "total = len(df)\n",
    "\n",
    "# Număr de favicon-uri găsite (logo_url nu e NaN)\n",
    "found = df[\"logo_url\"].notna().sum()\n",
    "\n",
    "# Afișează\n",
    "print(f\"{found} din {total} domenii au logo_url ({100 * found / total:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8385e9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rânduri: 4384\n",
      "Domenii unice: 3416\n",
      "Logo-uri găsite: 4259\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "favicons_df = pd.read_csv(\"favicons_result.csv\")\n",
    "logos_df = pd.read_csv(\"logos_detected_smart10.csv\")\n",
    "\n",
    "favicons_df[\"domain\"] = favicons_df[\"domain\"].str.strip().str.lower()\n",
    "logos_df[\"domain\"] = logos_df[\"domain\"].str.strip().str.lower()\n",
    "\n",
    "# Păstrează coloana 'domain' din favicons și completează 'logo_url' unde lipsește\n",
    "final_df = favicons_df.copy()\n",
    "final_df[\"logo_url\"] = final_df[\"logo_url\"].fillna(logos_df[\"logo_url\"])\n",
    "\n",
    "# Verificare\n",
    "print(f\"Total rânduri: {len(final_df)}\")\n",
    "print(f\"Domenii unice: {final_df['domain'].nunique()}\")\n",
    "print(f\"Logo-uri găsite: {final_df['logo_url'].notna().sum()}\")\n",
    "\n",
    "# Salvare\n",
    "final_df.to_csv(\"logos_finals.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "925eca81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4259 din 4384 domenii au logo_url (97.15%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"logos_finals.csv\")\n",
    "\n",
    "# Număr total\n",
    "total = len(df)\n",
    "\n",
    "# Număr de favicon-uri găsite (logo_url nu e NaN)\n",
    "found = df[\"logo_url\"].notna().sum()\n",
    "\n",
    "# Afișează\n",
    "print(f\"{found} din {total} domenii au logo_url ({100 * found / total:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be040da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rânduri de procesat: 4259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 4259/4259 [22:53<00:00,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Salvate: 4238 din 4259\n",
      " Eșecuri salvate în: failed_logos.csv\n",
      " Imagini salvate în: downloaded_logos_all\n",
      " Fișiere corupte salvate în: corrupt_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from io import BytesIO\n",
    "from urllib.parse import urlparse\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "import cairosvg\n",
    "import base64\n",
    "\n",
    "# Config\n",
    "CSV_FILE = \"logos_finals.csv\"\n",
    "FOLDER = \"downloaded_logos_all\"\n",
    "FAILED_CSV = \"failed_logos.csv\"\n",
    "CORRUPT_FOLDER = \"corrupt_images\"\n",
    "\n",
    "# Foldere\n",
    "os.makedirs(FOLDER, exist_ok=True)\n",
    "os.makedirs(CORRUPT_FOLDER, exist_ok=True)\n",
    "\n",
    "# Datele\n",
    "df = pd.read_csv(CSV_FILE)\n",
    "df = df.dropna(subset=[\"logo_url\", \"domain\"])\n",
    "df = df[df[\"logo_url\"].str.startswith((\"http\", \"data:image/\"))]\n",
    "\n",
    "# Eșecuri\n",
    "failed = []\n",
    "\n",
    "def get_extension(content_type, fallback=\".png\"):\n",
    "    if \"svg\" in content_type:\n",
    "        return \".svg\"\n",
    "    elif \"jpeg\" in content_type:\n",
    "        return \".jpg\"\n",
    "    elif \"png\" in content_type:\n",
    "        return \".png\"\n",
    "    elif \"gif\" in content_type:\n",
    "        return \".gif\"\n",
    "    else:\n",
    "        return fallback\n",
    "\n",
    "def generate_filename(domain, url, index, ext):\n",
    "    hashed = hashlib.md5(url.encode()).hexdigest()[:8]\n",
    "    name = f\"{domain}__{hashed}_{index}{ext}\"\n",
    "    return name.replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
    "\n",
    "def save_corrupt_image(content, filename):\n",
    "    path = os.path.join(CORRUPT_FOLDER, filename)\n",
    "    with open(path, \"wb\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "def download_image(domain, url, index):\n",
    "    try:\n",
    "        # Base64\n",
    "        if url.startswith(\"data:image\"):\n",
    "            header, b64data = url.split(\",\", 1)\n",
    "            ext = \".png\" if \"png\" in header else \".jpg\"\n",
    "            raw_data = base64.b64decode(b64data)\n",
    "            filename = generate_filename(domain, url, index, ext)\n",
    "            path = os.path.join(FOLDER, filename)\n",
    "            with open(path, \"wb\") as f:\n",
    "                f.write(raw_data)\n",
    "            return True, filename\n",
    "\n",
    "        # HTTP/HTTPS standard\n",
    "        r = requests.get(url, timeout=15, headers={\"User-Agent\": \"Mozilla/5.0\"}, verify=False)\n",
    "        if r.status_code != 200:\n",
    "            return False, f\"status {r.status_code}\"\n",
    "\n",
    "        content_type = r.headers.get(\"Content-Type\", \"\")\n",
    "        ext = get_extension(content_type)\n",
    "        filename = generate_filename(domain, url, index, ext)\n",
    "        save_path = os.path.join(FOLDER, filename)\n",
    "\n",
    "        if ext == \".svg\":\n",
    "            png_path = save_path.replace(\".svg\", \".png\")\n",
    "            cairosvg.svg2png(bytestring=r.content, write_to=png_path)\n",
    "            return True, png_path\n",
    "        else:\n",
    "            try:\n",
    "                image = Image.open(BytesIO(r.content))\n",
    "                if image.mode in (\"RGBA\", \"P\"):\n",
    "                    image = image.convert(\"RGBA\")\n",
    "                    image.save(save_path, format=\"PNG\")\n",
    "                else:\n",
    "                    image = image.convert(\"RGB\")\n",
    "                    image.save(save_path)\n",
    "                return True, filename\n",
    "            except UnidentifiedImageError:\n",
    "                # Salvează fișierul brut pentru analiză\n",
    "                bin_name = filename.replace(ext, \".bin\")\n",
    "                save_corrupt_image(r.content, bin_name)\n",
    "                return False, \"cannot identify image (saved as .bin)\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "# Procesare\n",
    "print(f\"Total rânduri de procesat: {len(df)}\")\n",
    "success_count = 0\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    domain = str(row[\"domain\"]).strip().lower()\n",
    "    url = str(row[\"logo_url\"]).strip()\n",
    "    ok, result = download_image(domain, url, index)\n",
    "    if ok:\n",
    "        success_count += 1\n",
    "    else:\n",
    "        failed.append({\"domain\": domain, \"logo_url\": url, \"error\": result})\n",
    "\n",
    "# Salvare\n",
    "if failed:\n",
    "    pd.DataFrame(failed).to_csv(FAILED_CSV, index=False)\n",
    "\n",
    "print(f\"\\n Salvate: {success_count} din {len(df)}\")\n",
    "print(f\" Eșecuri salvate în: {FAILED_CSV}\")\n",
    "print(f\" Imagini salvate în: {FOLDER}\")\n",
    "print(f\" Fișiere corupte salvate în: {CORRUPT_FOLDER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "194d68ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hashing imagini: 100%|████████████████████████████████████████████████████████████| 4240/4240 [00:07<00:00, 590.23it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import imagehash\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Config\n",
    "FOLDER = \"downloaded_logos_all\"\n",
    "HASH_THRESHOLD = 5\n",
    "CSV_FILE = \"logos_finals.csv\"\n",
    "\n",
    "# Hash-iesc imaginile\n",
    "hash_to_logos = defaultdict(list)\n",
    "\n",
    "for filename in tqdm(os.listdir(FOLDER), desc=\"Hashing imagini\"):\n",
    "    if not filename.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "        continue\n",
    "    path = os.path.join(FOLDER, filename)\n",
    "    try:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        phash = imagehash.phash(img)\n",
    "        domain = filename.split(\"__\")[0]\n",
    "        hash_to_logos[str(phash)].append((domain, filename))\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "# Grupez pe baza distanței hash\n",
    "visited = set()\n",
    "groups = []\n",
    "\n",
    "hashes = list(hash_to_logos.keys())\n",
    "for i, h1 in enumerate(hashes):\n",
    "    if h1 in visited:\n",
    "        continue\n",
    "    group = set(hash_to_logos[h1])\n",
    "    visited.add(h1)\n",
    "    for j in range(i + 1, len(hashes)):\n",
    "        h2 = hashes[j]\n",
    "        if h2 in visited:\n",
    "            continue\n",
    "        dist = imagehash.hex_to_hash(h1) - imagehash.hex_to_hash(h2)\n",
    "        if dist <= HASH_THRESHOLD:\n",
    "            group.update(hash_to_logos[h2])\n",
    "            visited.add(h2)\n",
    "    groups.append(sorted(list(group)))\n",
    "\n",
    "# Încarc logo_url din CSV-ul inițial\n",
    "df_meta = pd.read_csv(CSV_FILE)\n",
    "df_meta[\"domain\"] = df_meta[\"domain\"].str.strip().str.lower()\n",
    "domain_to_url = dict(zip(df_meta[\"domain\"], df_meta[\"logo_url\"]))\n",
    "\n",
    "# DataFrame-ul rezultat\n",
    "data = []\n",
    "for i, group in enumerate(groups):\n",
    "    for domain, fname in group:\n",
    "        data.append({\n",
    "            \"group_id\": i + 1,\n",
    "            \"domain\": domain,\n",
    "            \"filename\": fname,\n",
    "            \"logo_url\": domain_to_url.get(domain, \"\")\n",
    "        })\n",
    "\n",
    "df_groups = pd.DataFrame(data)\n",
    "\n",
    "# Sortare după group_id și domain\n",
    "df_groups = df_groups.sort_values(by=[\"group_id\", \"domain\"])\n",
    "\n",
    "# Salvare\n",
    "df_groups.to_csv(\"logo_similarity_groups.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
